# 第1章：介绍

本文档描述了PTX，一种低级并行线程执行虚拟机和指令集架构（ISA）。PTX将GPU作为数据并行计算设备展现出来。

## 1.1 使用GPU进行可扩展的数据并行计算

由于市场对实时、高清3D图形的持续需求，可编程GPU已发展成为一种高度并行、多线程、多核心的处理器，具有巨大的计算能力和非常高的内存带宽。GPU特别适合解决可以表达为数据并行计算的问题 - 同一程序在多个数据元素上并行执行 - 具有高算术强度 - 即算术运算与内存操作的比率。由于同一程序针对每个数据元素执行，对复杂流程控制的需求较低；由于在许多数据元素上执行并具有高算术强度，内存访问延迟可以通过计算来掩盖，而不是使用大型数据缓存。

数据并行处理将数据元素映射到并行处理线程。许多处理大型数据集的应用可以使用数据并行编程模型来加速计算。在3D渲染中，大量像素和顶点被映射到并行线程。类似地，图像和媒体处理应用，如渲染图像的后处理、视频编码和解码、图像缩放、立体视觉和模式识别，可以将图像块和像素映射到并行处理线程。事实上，许多图像渲染和处理领域之外的算法都通过数据并行处理得到加速，从通用信号处理或物理模拟到计算金融或计算生物学。

PTX定义了用于通用**并行线程执行**的虚拟机和ISA。PTX程序在安装时被翻译成目标硬件指令集。PTX-to-GPU翻译器和驱动程序使NVIDIA GPU能够作为可编程并行计算机使用。

## 1.2 PTX的目标

PTX提供了一个稳定的编程模型和指令集，用于通用并行编程。它旨在高效支持NVIDIA GPU上由NVIDIA Tesla架构定义的计算特性。诸如CUDA和C/C++等高级语言的编译器会生成PTX指令，这些指令经过优化并被翻译成本地目标架构指令。

PTX的目标包括以下几点：

- 提供一个跨越多代GPU的稳定指令集架构（ISA）。
- 在编译后的应用程序中实现与本地GPU性能相当的性能。
- 为C/C++和其他编译器提供一个与机器无关的指令集架构。
- 为应用程序和中间件开发者提供代码分发指令集架构。
- 提供一个通用的源级指令集架构，用于优化代码生成器和翻译器，将PTX映射到特定的目标机器。
- 便于手工编写库、高性能kernels和架构测试。
- 提供一个可扩展的编程模型，涵盖从单个单元到多个并行单元的GPU规模。

## 1.3 PTX ISA 8.5版本

PTX ISA 8.5版本引入了以下新特性：
- 增加了对`mma.sp::ordered_metadata`指令的支持。

## 1.4 文档结构

- **编程模型**：概述编程模型。
- **PTX机器模型**：提供PTX虚拟机模型的概览。
- **语法**：描述PTX语言的基本语法。
- **状态空间、类型和变量**：描述状态空间、类型和变量声明。
- **指令操作**：描述指令操作。
- **抽象ABI**：描述函数和调用语法、调用约定，以及PTX对抽象应用程序二进制接口（ABI）的支持。
- **指令集**：描述指令集。
- **特殊寄存器**：列出特殊寄存器。
- **指令**：列出PTX支持的汇编指令。
- 发布说明：提供PTX ISA 2.x及以后版本的发布说明。

## 参考文献

- IEEE 754-2008浮点运算标准。ISBN 978-0-7381-5752-8，2008年。
链接：http://ieeexplore.ieee.org/servlet/opac?punumber=4610933
- OpenCL规范，版本：1.1，文档修订：44，2011年6月1日。
链接：http://www.khronos.org/registry/cl/specs/opencl-1.1.pdf
- CUDA编程指南。
链接：https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html
- CUDA动态并行编程指南。
链接：https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-dynamic-parallelism
- CUDA原子性要求。
- 链接：https://nvidia.github.io/cccl/libcudacxx/extended_api/memory_model.html#atomicity
- PTX编写者互操作性指南。
链接：https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html

# 第2章：编程模型

## 2.1 高度多线程的协处理器

GPU是一种计算设备，能够并行执行大量线程。它作为主CPU（或称为主机）的协处理器运作。换句话说，运行在主机上的应用程序中数据并行、计算密集的部分被卸载到GPU设备上。

更准确地说，应用程序中需要多次执行但在不同数据上独立执行的部分，可以被隔离成一个内核函数，在GPU上以多个不同线程执行。为此，这样的函数被编译成PTX指令集，然后在安装时翻译成目标GPU的指令集。

## 2.2. 线程层次结构

执行kernel的线程批次被组织成一个网格。网格由协作线程数组或协作线程数组cluster组成，如本节所述并在图1和图2中说明。协作线程数组（CTAs）实现CUDA线程块，cluster实现cuda线程块cluster。

### 2.2.1. 协作线程数组

并行线程执行（PTX）编程模型是显式并行的：PTX程序指定并行线程数组中给定线程的执行。协作线程数组（CTA）是并发或并行执行kernel的线程数组。

CTA内的线程可以相互通信。为了协调CTA内线程的通信，可以指定同步点，在这些点上线程等待直到CTA中的所有线程都到达。

每个线程在CTA内有唯一的线程标识符。程序使用数据并行分解来在CTA的线程间划分输入、工作和结果。每个CTA线程使用其线程标识符来确定其分配的角色，分配特定的输入和输出位置，计算地址，并选择要执行的工作。线程标识符是一个三元素向量`tid`（包含`tid.x、tid.y`和`tid.z`元素），指定线程在1D、2D或3D CTA中的位置。每个线程标识符分量的范围从零到该CTA维度中的线程ID数量。

每个CTA有一个由三元向量`ntid`（包含`ntid.x、ntid.y`和`ntid.z`元素）指定的1D、2D或3D形状。向量`ntid`指定每个CTA维度中的线程数量。

CTA内的线程以SIMT（单指令多线程）方式执行，分组为称为`warps`的单位。一个`warp`是来自单个CTA的最大线程子集，这些线程同时执行相同的指令。`warp`内的线程按顺序编号。`warp`大小是依赖于机器的常量，通常一个`warp`有32个线程。一些应用程序可能能够通过了解`warp`大小来最大化性能，因此PTX包括一个运行时立即常量`WARP_SZ`，可在允许立即操作数的任何指令中使用。

### 2.2.2. 协作线程数组的cluster

cluster是一组并发或并行运行的CTA，它们可以通过共享内存进行同步和通信。执行CTA必须确保在通过共享内存与对等CTA通信之前，对等CTA的共享内存已存在，并且对等CTA在完成共享内存操作之前没有退出。

cluster中不同CTA的线程可以通过共享内存进行同步和通信。可以使用cluster范围的barrier来同步cluster内的所有线程。cluster中的每个CTA都有一个在其cluster内唯一的CTA标识符（`cluster_ctaid`）。每个CTAcluster有由参数`cluster_nctaid`指定的1D、2D或3D形状。cluster中的每个CTA还有一个跨所有维度唯一的CTA标识符（`cluster_ctarank`）。cluster中所有维度的CTA总数由`cluster_nctarank`指定。线程可以通过预定义的只读特殊寄存器`%cluster_ctaid、%cluster_nctaid、%cluster_ctarank、%cluster_nctarank`读取和使用这些值。

cluster级别仅适用于目标架构sm_90或更高版本。在启动时指定cluster维度是可选的。如果用户在启动时指定了cluster维度，则会被视为显式cluster启动，否则将被视为隐式cluster启动，默认维度为`1x1x1`。PTX提供只读特殊寄存器`%is_explicit_cluster`来区分显式和隐式cluster启动。

### 2.2.3. cluster网格

一个CTA可以包含的线程数量和一个cluster可以包含的CTA数量都有上限。然而，执行相同 kernel 的CTA clusters可以被批处理成一个cluster网格，这样在单次kernel调用中可以启动的线程总数就非常大。这样做的代价是减少了线程间的通信和同步能力，因为不同cluster中的线程无法相互通信和同步。

在cluster网格中，每个cluster有一个唯一的cluster标识符（`clusterid`）。每个cluster网格由参数`nclusterid`指定1D、2D或3D形状。每个网格还有一个唯一的时间标识符（`gridid`）。线程可以通过预定义的只读特殊寄存器`%tid、%ntid、%clusterid、%nclusterid`和`%gridid`读取和使用这些值。

在一个网格中，每个CTA有一个唯一的标识符（`ctaid`）。每个CTA网格由参数`nctaid`指定1D、2D或3D形状。线程可以通过预定义的只读特殊寄存器`%ctaid`和`%nctaid`使用和读取这些值。

每个kernel被执行为一个由CTA组成的cluster网格形式的线程批次，其中cluster是可选级别，仅适用于sm_90及更高的目标架构。图1展示了由CTA组成的网格，图2展示了由cluster组成的网格。

网格可以在彼此之间具有依赖关系 - 一个网格可能是依赖网格和/或先决条件网格。要了解如何定义网格依赖关系，请参阅《CUDA编程指南》中的CUDA Graph部分。

![](https://files.mdnice.com/user/59/7eeb692b-170a-41f3-a8dd-020751a5eb46.png)

![](https://files.mdnice.com/user/59/69dca117-809c-4358-a830-5ad63cd7eddd.png)

## 2.3. 内存层次结构

PTX线程在执行过程中可以访问多个状态空间的数据，如图3所示，其中cluster级别从目标架构sm_90开始引入。每个线程都有私有的本地内存。每个线程块（CTA）有一个共享内存，对该块内的所有线程以及cluster中所有活动块可见，其生命周期与块相同。最后，所有线程都可以访问相同的全局内存。

![](https://files.mdnice.com/user/59/c94ac3b1-506b-4cee-a3f5-6f5cf083e9f8.png)

所有线程还可以访问其他状态空间：常量、参数、纹理和表面状态空间。常量和纹理内存是只读的；表面内存可读可写。全局、常量、参数、纹理和表面状态空间针对不同的内存用途进行了优化。例如，纹理内存提供不同的寻址模式和特定数据格式的数据过滤。注意，纹理和表面内存是有缓存的，在同一个kernel调用中，缓存不会与全局内存写入和表面内存写入保持一致。

因此，对于在同一kernel调用中通过全局或表面写入已写入的地址进行的任何纹理获取或表面读取都将返回未定义的数据。
> 这句话不是好翻译。就是下面换句话说表达的意思。

**换句话说，线程只有在该内存位置已被先前的kernel调用或内存复制更新时，才能安全地读取某个纹理或表面内存位置，而不能是被同一线程或同一kernel调用中的另一个线程先前更新过的**。

全局、常量和纹理状态空间在同一应用程序的kernel启动之间是持久的。主机和设备都维护自己的本地内存，分别称为主机内存和设备内存。设备内存可以被主机映射并读写，或者为了更高效的传输，通过利用设备的高性能Direct Memory Access（DMA）引擎的优化API调用从主机内存复制。

# 第三章：PTX机器模型

## 3.1 SIMT  SM (SMs) 集合

NVIDIA GPU架构建立在可扩展的多线程流式 SM（SMs）阵列之上。当主机程序调用kernel网格时，网格的块被枚举并分配给有可用执行能力的 SM。线程块的线程在一个 SM上并发执行。当线程块终止时，新的块会在空闲的 SM上启动。

 SM由多个标量处理器（SP）核心、多线程指令单元和片上共享内存组成。 SM在硬件中创建、管理和执行并发线程，无调度开销。它实现了单指令屏障同步。快速屏障同步与轻量级线程创建和零开销线程调度一起，有效支持非常细粒度的并行性，例如，允许通过为每个数据元素（如图像中的像素、体积中的体素、基于网格计算中的单元）分配一个线程来实现问题的低粒度分解。

为了管理运行多个不同程序的数百个线程， SM采用了我们称为SIMT（单指令多线程）的架构。 SM将每个线程映射到一个标量处理器核心，每个标量线程独立执行，有自己的指令地址和寄存器状态。 SM的SIMT单元创建、管理、调度和执行称为`warps`的并行线程组。（这个术语源自织布，即第一个并行线程技术。）组成SIMT warp的单个线程在同一程序地址开始，但可以自由分支和独立执行。

当 SM被给予一个或多个线程块来执行时，它将它们分成由SIMT单元调度的warps。块分割成warps的方式总是相同的；每个warp包含连续递增线程ID的线程，第一个warp包含线程0。

在每个指令发出时，SIMT单元选择一个准备执行的warp，并向该warp的活动线程发出下一条指令。一个warp一次执行一条共同指令，当warp中的所有线程在执行路径上一致时，可以实现完全效率。如果warp中的线程通过数据依赖的条件分支发生分歧，warp将串行执行每个分支路径，禁用不在该路径上的线程，当所有路径完成时，线程会重新汇合到相同的执行路径。分支分歧只发生在warp内；不同的warps独立执行，无论它们是执行共同还是不相交的代码路径。

SIMT架构类似于SIMD（单指令多数据）向量组织，因为单个指令控制多个处理元素。关键区别在于SIMD向量组织向软件暴露SIMD宽度，而SIMT指令指定单个线程的执行和分支行为。与SIMD向量机不同，SIMT使程序员能够为独立的标量线程以及协调线程编写线程级并行代码。为了保证正确性，程序员可以基本忽略SIMT行为；然而，通过注意确保warp中的线程很少发生分歧，可以实现显著的性能改进。在实践中，这类似于传统代码中缓存行的作用：在设计正确性时可以安全忽略缓存行大小，但在设计峰值性能时必须在代码结构中考虑。另一方面，向量架构要求软件将负载合并成向量并手动管理分歧。

 SM一次可以处理多少个块取决于每个线程需要多少寄存器以及每个块需要多少共享内存，因为 SM的寄存器和共享内存在一批块的所有线程之间分配。如果每个 SM没有足够的寄存器或共享内存来处理至少一个块，kernel将无法启动。

![](https://files.mdnice.com/user/59/aa599b8d-25bf-4471-aceb-4b9add195b86.png)

## 独立线程调度

在Volta架构之前，warp使用一个单一的程序计数器，由warp中所有32个线程共享，同时使用一个活动掩码来指定warp中的活动线程。因此，来自同一warp的线程在分支区域或不同执行状态下无法相互发信号或交换数据，需要细粒度数据共享的算法可能容易导致死锁，这取决于竞争线程来自哪个warp。

从Volta架构开始，独立线程调度允许线程之间的完全并发，不受warp限制。通过独立线程调度，GPU为每个线程维护执行状态，包括程序计数器和调用栈，可以实现每线程粒度的执行，以更好地利用执行资源或允许一个线程等待另一个线程产生数据。调度优化器决定如何将同一warp中的活动线程分组到SIMT单元中。这保留了之前NVIDIA GPU中SIMT执行的高吞吐量，但具有更大的灵活性：线程现在可以在sub-warp粒度级别上发生分歧和重新汇合。

如果开发者对之前硬件架构的warp同步性做出了假设，独立线程调度可能会导致参与执行代码的线程集与预期不同。特别是，任何warp同步代码（如无同步的warp内规约）都应该重新检查，以确保与Volta及以后架构的兼容性。更多详细信息请参阅CUDA编程指南中的Compute Capability 7.x部分。

## 片上共享内存

如图4所示，每个SM都有四种类型的片上内存：

- 每个处理器一组32位本地寄存器，
- 所有标量处理器核心共享的并行数据缓存或共享内存，这也是共享内存空间所在的地方，
- 所有标量处理器核心共享的只读常量缓存，用于加速从常量内存空间（设备内存的只读区域）的读取，
- 所有标量处理器核心共享的只读纹理缓存，用于加速从纹理内存空间（设备内存的只读区域）的读取；每个SM通过纹理单元访问纹理缓存，该单元实现各种寻址模式和数据过滤。

本地和全局内存空间是设备内存的读写区域。

# 第4章：语法

PTX程序是一组文本源模块（文件）的集合。PTX源模块采用汇编语言风格的语法，包含指令操作码和操作数。伪操作指定符号和寻址管理。`ptxas` 优化后端编译器优化并汇编 PTX 源模块，以生成相应的二进制对象文件。

## 4.1. 源格式

源模块是ASCII文本。行由换行符(\n)分隔。

所有空白字符都是等效的；空白被忽略，除非用于分隔语言中的tokens。

可以使用C预处理器cpp来处理PTX源模块。以#开头的行是预处理器指令。以下是常见的预处理器指令：

`#include, #define, #if, #ifdef, #else, #endif, #line, #file`

> Harbison和Steele的《C语言参考手册》对C预处理器有很好的描述。

PTX区分大小写，并对关键字使用小写。

每个PTX模块必须以.version指令开始，指定PTX语言版本，然后是`.target`指令，指定假定的目标架构。有关这些指令的更多信息，请参阅PTX模块指令 (这个文档的第11章)。

## 4.2. 注释

PTX中的注释遵循C/C++语法，使用非嵌套的/*和*/用于可能跨越多行的注释，使用//开始一个延伸到下一个换行符的注释，换行符终止当前行。注释不能出现在字符常量、字符串字面量或其他注释内。

PTX中的注释被视为空白。

## 4.3 Statements(语句)

PTX Statements要么是directive，要么是instruction。Statements以可选的标签开始，以分号结束。

示例：

```apache
.reg    .b32 r1, r2;
.global .f32 array[N];

start:  mov.b32  r1, %tid.x;
        shl.b32  r1, r1, 2;        // 将线程id左移2位
        ld.global.b32 r2, array[r1]; // thread[tid] 获取 array[tid]
        add.f32  r2, r2, 0.5;      // 加 1/2
```

### 4.3.1 Directive Statements

Directive关键字以点开头，因此不可能与用户定义的标识符冲突。PTX中的Directive列在表1中，并在"状态空间、类型和变量"和"Directives"章节中描述。

![](https://files.mdnice.com/user/59/4a74823b-d74b-4ef1-ae97-389b33521693.png)

### 4.3.2 Instruction Statements

Instruction由**指令操作码**后跟逗号分隔的**零个或多个操作数列表**组成，以分号结束。操作数可以是寄存器变量、常量表达式、地址表达式或标签名称。指令可以有一个可选的守卫谓词，用于条件执行。守卫谓词跟在可选标签之后，位于操作码之前，写作@p，其中p是谓词寄存器。守卫谓词可以选择性地取反，写作@!p。

目标操作数在前，后跟源操作数。

Instruction关键字列在表2中。所有Instruction关键字在PTX中都是保留tokens。

![](https://files.mdnice.com/user/59/bb93e069-934c-461b-a4a7-f39ecc56ead7.png)


## 4.4 标识符

用户定义的标识符遵循扩展的C++规则：它们要么以字母开头，后跟零个或多个字母、数字、下划线或美元符号；要么以下划线、美元符号或百分号开头，后跟一个或多个字母、数字、下划线或美元符号：

```apache
followsym: [a-zA-Z0-9_$]
identifier: [a-zA-Z]{followsym}* | {[_$%]{followsym}+
```

PTX未指定标识符的最大长度，并建议所有实现至少支持1024个字符的最小长度。

许多高级语言（如C和C++）遵循类似的标识符命名规则，但不允许使用百分号。PTX允许百分号作为标识符的第一个字符。百分号可用于避免名称冲突，例如，在用户定义的变量名和编译器生成的名称之间。

PTX预定义了一个常量和少量以百分号开头的特殊寄存器，列在表3中。

![](https://files.mdnice.com/user/59/6e425754-3231-48f5-9289-0c7bb403d237.png)

## 4.5 常量

PTX支持整数和浮点常量以及常量表达式。这些常量可用于数据初始化和作为指令的操作数。类型检查规则对整数、浮点和位大小类型保持不变。对于谓词类型的数据和指令，允许使用整数常量，并按C语言的方式解释，即零值为False，非零值为True。

## 4.5.1. 整数常量

整数常量是64位大小，可以是有符号或无符号的，即每个整数常量都有`.s64`或`.u64`类型。有符号/无符号性质对于正确评估包含除法和有序比较等操作的常量表达式是必需的，其中操作的行为取决于操作数类型。当用于指令或数据初始化时，每个整数常量会被转换为基于数据或指令类型的适当大小。

整数字面量可以用十进制、十六进制、八进制或二进制表示法书写。语法遵循C语言。整数字面量后可以立即跟上字母U，表示该字面量是无符号的。

```shell
十六进制字面量：0[xX]{hexdigit}+U?
八进制字面量： 0(octal digit)+U?
二进制字面量： 0[bB]{bit}+U?
十进制字面量： (nonzero-digit)(digit)*U?
```

整数字面量是非负的，其类型由其大小和可选的类型后缀决定，如下所示：字面量是有符号的（`.s64`），除非该值不能完全表示为`.s64`，或者指定了无符号后缀，在这种情况下字面量是无符号的（`.u64`）。

预定义的整数常量`WARP_SZ`指定目标平台的每个warp的线程数；到目前为止，所有目标架构的`WARP_SZ`值都是32。













